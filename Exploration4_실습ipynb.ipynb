{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exploration4 실습ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM1CO/1rGCPPznYh12ImQw6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeongUgSeo/Exploration4_basic/blob/main/Exploration4_%EC%8B%A4%EC%8A%B5ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibj3v2r9XazU",
        "outputId": "66beed00-8852-4870-a75b-c7d7532e15ad"
      },
      "source": [
        "import os, re \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 파일을 읽기모드로 열고\n",
        "# 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
        "file_path = '/content/shakespeare.txt'\n",
        "with open(file_path, \"r\") as f:\n",
        "    raw_corpus = f.read().splitlines()\n",
        "\n",
        "# 앞에서부터 10라인만 화면에 출력해 볼까요?\n",
        "print(raw_corpus[:9])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft8LYPxuXulI",
        "outputId": "f6ed0fee-9f1d-4b49-d43c-741d825713fc"
      },
      "source": [
        "for idx, sentence in enumerate(raw_corpus):\n",
        "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
        "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
        "\n",
        "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
        "        \n",
        "    print(sentence)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before we proceed any further, hear me speak.\n",
            "Speak, speak.\n",
            "You are all resolved rather to die than to famish?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppy7b_QFXxeV",
        "outputId": "fafe014c-ffc0-40c7-d51f-b3579e2fc441"
      },
      "source": [
        "# 입력된 문장을\n",
        "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "#     2. 특수문자 양쪽에 공백을 넣고\n",
        "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "#     5. 다시 양쪽 공백을 지웁니다\n",
        "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip() # 5\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
        "    return sentence\n",
        "\n",
        "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "annyH94uXzhP",
        "outputId": "bffede11-9b01-4fac-f52d-d53a868406ae"
      },
      "source": [
        "# 여기에 정제된 문장을 모을겁니다\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    # 정제를 하고 담아주세요\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "        \n",
        "# 정제된 결과를 10개만 확인해보죠\n",
        "corpus[:10]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> before we proceed any further , hear me speak . <end>',\n",
              " '<start> speak , speak . <end>',\n",
              " '<start> you are all resolved rather to die than to famish ? <end>',\n",
              " '<start> resolved . resolved . <end>',\n",
              " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
              " '<start> we know t , we know t . <end>',\n",
              " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
              " '<start> is t a verdict ? <end>',\n",
              " '<start> no more talking on t let it be done away , away ! <end>',\n",
              " '<start> one word , good citizens . <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtkINBr3X16K",
        "outputId": "9702743e-5278-417e-b358-7bcf536039e6"
      },
      "source": [
        "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
        "# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
        "def tokenize(corpus):\n",
        "    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
        "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
        "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=7000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
        "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
        "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2  143   40 ...    0    0    0]\n",
            " [   2  110    4 ...    0    0    0]\n",
            " [   2   11   50 ...    0    0    0]\n",
            " ...\n",
            " [   2  149 4553 ...    0    0    0]\n",
            " [   2   34   71 ...    0    0    0]\n",
            " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fc1849caed0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnejFK0TX4so",
        "outputId": "731c2432-96d7-40e6-9966-7883a975f6c8"
      },
      "source": [
        "print(tensor[:3, :10])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2  143   40  933  140  591    4  124   24  110]\n",
            " [   2  110    4  110    5    3    0    0    0    0]\n",
            " [   2   11   50   43 1201  316    9  201   74    9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEe_ywYLX790",
        "outputId": "4ab584d3-09c7-4fc6-969f-4d831dd354fa"
      },
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : .\n",
            "6 : the\n",
            "7 : and\n",
            "8 : i\n",
            "9 : to\n",
            "10 : of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUVlu6J1X9oY",
        "outputId": "ea269eaf-1584-4f21-bf5a-921295a95580"
      },
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
        "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
        "src_input = tensor[:, :-1]  \n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
            "   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVsoo6fhX_UX",
        "outputId": "0ceadbc5-4c52-4c32-9f32-156cdbc48ccf"
      },
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
        "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
        "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn4_6Ru4YCN6"
      },
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye2oowueYHYW",
        "outputId": "07f4f031-730f-4dd3-ac9b-d4060954e98a"
      },
      "source": [
        "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
        "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "model(src_sample)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
              "array([[[ 1.32420988e-04, -1.48806703e-05,  1.42032164e-04, ...,\n",
              "         -6.47871493e-05, -2.53340637e-04,  1.46318518e-04],\n",
              "        [ 1.02330720e-04,  3.56400233e-05,  3.78641998e-04, ...,\n",
              "         -2.38070988e-05, -1.49253887e-04,  3.97253141e-04],\n",
              "        [ 5.85719063e-05,  6.56582124e-05,  5.17343986e-04, ...,\n",
              "          9.21121682e-05,  2.04945754e-04,  5.45668532e-04],\n",
              "        ...,\n",
              "        [-2.32734671e-03,  2.14986387e-03, -2.78481725e-03, ...,\n",
              "          2.10880477e-04,  1.40099030e-03,  2.70667428e-04],\n",
              "        [-2.70505832e-03,  2.26058974e-03, -3.19411024e-03, ...,\n",
              "          1.48863226e-04,  1.42259861e-03,  3.12452990e-04],\n",
              "        [-3.04443785e-03,  2.35006004e-03, -3.54398880e-03, ...,\n",
              "          1.17411146e-04,  1.43902830e-03,  3.37955280e-04]],\n",
              "\n",
              "       [[ 1.32420988e-04, -1.48806703e-05,  1.42032164e-04, ...,\n",
              "         -6.47871493e-05, -2.53340637e-04,  1.46318518e-04],\n",
              "        [ 5.46519514e-05,  4.47862200e-04,  2.11466453e-04, ...,\n",
              "         -8.45372560e-05, -2.27458790e-04, -2.36439970e-04],\n",
              "        [ 1.36309463e-04,  4.90572304e-04,  8.17133696e-04, ...,\n",
              "         -1.06635402e-04, -3.55997356e-04, -2.40822497e-04],\n",
              "        ...,\n",
              "        [-2.15274328e-03,  2.21806485e-03, -2.68322020e-03, ...,\n",
              "         -5.90943557e-04,  8.10189580e-04, -1.34489179e-04],\n",
              "        [-2.56073242e-03,  2.24000239e-03, -3.03597166e-03, ...,\n",
              "         -5.11584163e-04,  8.29313940e-04,  4.84454449e-06],\n",
              "        [-2.93201488e-03,  2.26623379e-03, -3.34503385e-03, ...,\n",
              "         -4.23142541e-04,  8.59111722e-04,  1.12580143e-04]],\n",
              "\n",
              "       [[ 1.32420988e-04, -1.48806703e-05,  1.42032164e-04, ...,\n",
              "         -6.47871493e-05, -2.53340637e-04,  1.46318518e-04],\n",
              "        [-3.25585024e-05,  7.91078637e-05,  1.72469925e-04, ...,\n",
              "          1.80031911e-05, -1.34161337e-05,  4.67862294e-04],\n",
              "        [ 6.58786681e-04, -2.04806100e-04,  1.90332124e-04, ...,\n",
              "          1.69861785e-04,  1.45869053e-04,  4.85619094e-04],\n",
              "        ...,\n",
              "        [-2.77286838e-03,  1.77412515e-03, -3.65923066e-03, ...,\n",
              "         -4.78176895e-04,  1.35850289e-03, -4.86172758e-05],\n",
              "        [-3.09097674e-03,  1.89384993e-03, -3.90376081e-03, ...,\n",
              "         -3.80159909e-04,  1.36893929e-03,  3.02199951e-05],\n",
              "        [-3.35895410e-03,  2.00855848e-03, -4.10862640e-03, ...,\n",
              "         -2.83230736e-04,  1.37596682e-03,  9.59427562e-05]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 1.32420988e-04, -1.48806703e-05,  1.42032164e-04, ...,\n",
              "         -6.47871493e-05, -2.53340637e-04,  1.46318518e-04],\n",
              "        [ 3.30790863e-05,  9.48294764e-05,  1.00364001e-03, ...,\n",
              "         -3.16867692e-04, -5.19557390e-04,  4.04874154e-05],\n",
              "        [ 5.42105154e-05, -6.45476612e-05,  1.28697162e-03, ...,\n",
              "         -4.88552760e-05, -5.30181045e-04,  1.46965365e-04],\n",
              "        ...,\n",
              "        [-3.70845012e-03,  2.17916351e-03, -4.18701535e-03, ...,\n",
              "          5.22314978e-04,  1.24162354e-03,  6.94988645e-04],\n",
              "        [-3.86015372e-03,  2.27639382e-03, -4.33524046e-03, ...,\n",
              "          5.16453874e-04,  1.28251605e-03,  6.63143466e-04],\n",
              "        [-3.96862859e-03,  2.36145779e-03, -4.45625558e-03, ...,\n",
              "          5.24374016e-04,  1.32190145e-03,  6.37046527e-04]],\n",
              "\n",
              "       [[ 1.32420988e-04, -1.48806703e-05,  1.42032164e-04, ...,\n",
              "         -6.47871493e-05, -2.53340637e-04,  1.46318518e-04],\n",
              "        [ 3.75626580e-04,  6.49551439e-05, -1.70526924e-04, ...,\n",
              "          3.14324920e-04, -3.68538924e-04,  3.69798276e-04],\n",
              "        [ 7.82697112e-04,  1.31495544e-04, -1.46867344e-04, ...,\n",
              "          4.85260942e-04, -1.63567966e-04,  3.31528019e-04],\n",
              "        ...,\n",
              "        [-3.81408748e-03,  2.39851233e-03, -4.53110225e-03, ...,\n",
              "          4.21834877e-04,  1.43287727e-03,  6.24896144e-04],\n",
              "        [-3.90860997e-03,  2.46764091e-03, -4.62238910e-03, ...,\n",
              "          4.43783734e-04,  1.46089552e-03,  6.01683103e-04],\n",
              "        [-3.97314643e-03,  2.52705137e-03, -4.69522830e-03, ...,\n",
              "          4.74193686e-04,  1.48753030e-03,  5.85428439e-04]],\n",
              "\n",
              "       [[ 1.32420988e-04, -1.48806703e-05,  1.42032164e-04, ...,\n",
              "         -6.47871493e-05, -2.53340637e-04,  1.46318518e-04],\n",
              "        [-1.07829859e-04, -1.31017630e-04,  3.34404816e-04, ...,\n",
              "         -2.43758870e-04, -1.73674984e-04,  2.25663389e-04],\n",
              "        [-5.36038657e-04, -8.00662965e-05,  3.58673395e-04, ...,\n",
              "         -6.62256614e-04, -2.15476364e-04,  3.31364368e-04],\n",
              "        ...,\n",
              "        [-9.45943466e-04,  2.00891844e-03, -3.06465803e-03, ...,\n",
              "         -1.36967236e-03,  2.08481192e-03, -4.58817318e-04],\n",
              "        [-1.41770858e-03,  2.13696016e-03, -3.38449725e-03, ...,\n",
              "         -1.23197853e-03,  2.10457761e-03, -3.13431898e-04],\n",
              "        [-1.86061510e-03,  2.23948411e-03, -3.66429682e-03, ...,\n",
              "         -1.08639395e-03,  2.09043175e-03, -1.95202316e-04]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ucv23PaYvje",
        "outputId": "3715bffe-558a-4736-c015-7c347eb16ce6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  1792256   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  multiple                  5246976   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                multiple                  8392704   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  7176025   \n",
            "=================================================================\n",
            "Total params: 22,607,961\n",
            "Trainable params: 22,607,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hJx7D0wYz1N",
        "outputId": "c62c4d81-77ce-4d31-ab5e-d2b9a6890dd7"
      },
      "source": [
        "# optimizer와 loss등은 차차 배웁니다\n",
        "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
        "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(dataset, epochs=3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "93/93 [==============================] - 36s 360ms/step - loss: 3.0196\n",
            "Epoch 2/3\n",
            "93/93 [==============================] - 33s 358ms/step - loss: 2.7188\n",
            "Epoch 3/3\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 2.6000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc183a1bf50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuGyRKE4Y1OP"
      },
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_ul_RiYsY6XG",
        "outputId": "55ced3cd-cf92-4b3b-e381-a6e81aaa7c23"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> he , i ll not , and i have be , <end> '"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8SLx5AxY8RC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}